{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819144ad",
   "metadata": {},
   "source": [
    "# MedGemma 4B LoRA Fine‑Tuning (FP16/BF16, No Quantization)\n",
    "\n",
    "**Notebook version of `train_lora_4b.py`** — split into clean, runnable sections.\n",
    "\n",
    "## What this does\n",
    "- Loads **MedGemma 1.5 4B IT** in **fp16/bf16** (no 4‑bit / bitsandbytes)\n",
    "- Adds **LoRA adapters** (PEFT)\n",
    "- Uses **assistant‑only loss masking** (labels = -100 on prompt tokens)\n",
    "- Uses **pad‑safe collator** (`labels` padded with -100)\n",
    "- Adds **token_type_ids** (required for Gemma3 training)\n",
    "- Avoids LoRA injection into the vision tower by filtering module paths\n",
    "- Hard‑disables `use_cache` everywhere + uses non‑reentrant gradient checkpointing where possible\n",
    "\n",
    "## Prereqs\n",
    "- CUDA GPU required\n",
    "- Your dataset JSONL files must have fields: `prompt`, `target`\n",
    "- You must have **`hf_auth.py`** available (same folder or in `PYTHONPATH`) providing:\n",
    "  - `get_hf_token(hf_token_arg: str) -> str`\n",
    "  - `try_with_token(fn, *args, token=..., **kwargs)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8575c",
   "metadata": {},
   "source": [
    "## 1) Install dependencies (optional)\n",
    "If you're in a fresh environment, run this cell once.\n",
    "> If you already have these installed, you can skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed (uncomment):\n",
    "# %pip install -U \"torch\" \"transformers\" \"datasets\" \"peft\" \"numpy\"\n",
    "\n",
    "# Optional sanity prints:\n",
    "import sys, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ddde72",
   "metadata": {},
   "source": [
    "## 2) Configuration\n",
    "Edit these values instead of CLI flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# CONFIG (edit me)\n",
    "# =============================\n",
    "CFG = dict(\n",
    "    train_jsonl=\"data/train.jsonl\",\n",
    "    val_jsonl=\"data/val.jsonl\",\n",
    "    model_name=\"google/medgemma-1.5-4b-it\",\n",
    "    out_dir=\"medgemma4b_icu_lora\",\n",
    "\n",
    "    max_len=1024,\n",
    "    eval_max_len=0,      # 0 => use max_len; set smaller to reduce eval VRAM\n",
    "    epochs=1,\n",
    "    lr=2e-4,\n",
    "    batch=1,\n",
    "    grad_accum=16,\n",
    "    seed=7,\n",
    "\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "\n",
    "    eval_steps=0,        # 0 => eval each epoch; otherwise eval every N steps\n",
    "    hf_token=\"\",         # or set HF_TOKEN env var; see hf_auth.py behavior\n",
    "    warmup_ratio=0.03,\n",
    "    scheduler=\"cosine\",  # cosine | linear | constant\n",
    "    log_steps=10,\n",
    "    save_merged=False,\n",
    "\n",
    "    no_eval=False,       # True => disable eval to avoid eval OOM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec63b4",
   "metadata": {},
   "source": [
    "## 3) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForImageTextToText,\n",
    "    AutoProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from hf_auth import get_hf_token, try_with_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80329623",
   "metadata": {},
   "source": [
    "## 4) Repro + dtype helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def pick_compute_dtype():\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "        return torch.bfloat16\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.float16\n",
    "    return torch.float32\n",
    "\n",
    "\n",
    "def print_gpu_mem(prefix=\"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    try:\n",
    "        torch.cuda.synchronize()\n",
    "        a = torch.cuda.memory_allocated() / (1024**3)\n",
    "        r = torch.cuda.memory_reserved() / (1024**3)\n",
    "        m = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "        print(f\"{prefix}CUDA mem: allocated={a:.2f}GB reserved={r:.2f}GB max_alloc={m:.2f}GB\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b514eb",
   "metadata": {},
   "source": [
    "## 5) Gradient checkpointing + `use_cache` hard-disable\n",
    "These cells replicate the warning fixes from your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# PyTorch checkpoint warning fix (PyTorch 2.9+ may require explicit use_reentrant)\n",
    "# ------------------------------------------------------------\n",
    "def patch_torch_checkpoint_default_use_reentrant_false():\n",
    "    try:\n",
    "        import inspect\n",
    "        import torch.utils.checkpoint as ckpt\n",
    "\n",
    "        sig = inspect.signature(ckpt.checkpoint)\n",
    "        if \"use_reentrant\" not in sig.parameters:\n",
    "            return\n",
    "\n",
    "        if getattr(ckpt.checkpoint, \"_patched_use_reentrant_default\", False):\n",
    "            return\n",
    "\n",
    "        _orig = ckpt.checkpoint\n",
    "\n",
    "        def _wrapped(function, *args, **kwargs):\n",
    "            kwargs.setdefault(\"use_reentrant\", False)\n",
    "            return _orig(function, *args, **kwargs)\n",
    "\n",
    "        _wrapped._patched_use_reentrant_default = True\n",
    "        ckpt.checkpoint = _wrapped\n",
    "    except Exception:\n",
    "        return\n",
    "\n",
    "\n",
    "def enable_gc_no_reentrant(model):\n",
    "    if not hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "        return\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    except TypeError:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "def force_disable_use_cache_everywhere(model):\n",
    "    \"\"\"Recursively forces use_cache=False anywhere found.\"\"\"\n",
    "    visited = set()\n",
    "\n",
    "    def _set_use_cache(obj):\n",
    "        if obj is None:\n",
    "            return\n",
    "        oid = id(obj)\n",
    "        if oid in visited:\n",
    "            return\n",
    "        visited.add(oid)\n",
    "\n",
    "        if hasattr(obj, \"use_cache\"):\n",
    "            try:\n",
    "                obj.use_cache = False\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        for attr in (\n",
    "            \"config\",\n",
    "            \"generation_config\",\n",
    "            \"text_config\",\n",
    "            \"language_config\",\n",
    "            \"vision_config\",\n",
    "            \"model_config\",\n",
    "        ):\n",
    "            if hasattr(obj, attr):\n",
    "                try:\n",
    "                    _set_use_cache(getattr(obj, attr))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    _set_use_cache(model)\n",
    "    for attr in (\"model\", \"base_model\", \"language_model\", \"vision_tower\", \"vision_model\"):\n",
    "        if hasattr(model, attr):\n",
    "            try:\n",
    "                _set_use_cache(getattr(model, attr))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    try:\n",
    "        for m in model.modules():\n",
    "            if hasattr(m, \"config\"):\n",
    "                _set_use_cache(m.config)\n",
    "            if hasattr(m, \"generation_config\"):\n",
    "                _set_use_cache(m.generation_config)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def suppress_use_cache_gc_warning():\n",
    "    \"\"\"Silence the specific transformers logger warning during GC enable.\"\"\"\n",
    "    names = [\"transformers.modeling_utils\", \"transformers\"]\n",
    "    loggers = [logging.getLogger(n) for n in names]\n",
    "    old_levels = [lg.level for lg in loggers]\n",
    "    try:\n",
    "        for lg in loggers:\n",
    "            lg.setLevel(logging.ERROR)\n",
    "        yield\n",
    "    finally:\n",
    "        for lg, lvl in zip(loggers, old_levels):\n",
    "            lg.setLevel(lvl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913185d1",
   "metadata": {},
   "source": [
    "## 6) Chat-template tokenization (assistant-only loss)\n",
    "This keeps your exact masking logic and adds `token_type_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _messages_text_only(prompt: str, target: str | None = None) -> List[Dict[str, Any]]:\n",
    "    msgs = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "    if target is not None:\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": target}]})\n",
    "    return msgs\n",
    "\n",
    "\n",
    "def _ensure_1d_list(x):\n",
    "    if isinstance(x, dict) and \"input_ids\" in x:\n",
    "        x = x[\"input_ids\"]\n",
    "    if torch.is_tensor(x):\n",
    "        x = x.tolist()\n",
    "    if isinstance(x, list) and len(x) > 0 and isinstance(x[0], list):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def _apply_chat(processor, messages, max_len: int, add_generation_prompt: bool):\n",
    "    if not hasattr(processor, \"apply_chat_template\"):\n",
    "        raise RuntimeError(\"Processor does not support apply_chat_template; please upgrade transformers.\")\n",
    "\n",
    "    out = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "    )\n",
    "\n",
    "    input_ids = _ensure_1d_list(out[\"input_ids\"])\n",
    "    attn = out.get(\"attention_mask\", None)\n",
    "    attn = _ensure_1d_list(attn) if attn is not None else None\n",
    "    if attn is None:\n",
    "        attn = [1] * len(input_ids)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attn}\n",
    "\n",
    "\n",
    "def tokenize_fn(processor, example, max_len: int):\n",
    "    prompt = example[\"prompt\"]\n",
    "    target = example[\"target\"]\n",
    "\n",
    "    full_msgs = _messages_text_only(prompt, target)\n",
    "    prompt_msgs = _messages_text_only(prompt, None)\n",
    "\n",
    "    full = _apply_chat(processor, full_msgs, max_len=max_len, add_generation_prompt=False)\n",
    "    pref = _apply_chat(processor, prompt_msgs, max_len=max_len, add_generation_prompt=True)\n",
    "\n",
    "    input_ids = full[\"input_ids\"]\n",
    "    attention_mask = full[\"attention_mask\"]\n",
    "\n",
    "    # Gemma3 requires token_type_ids during training; text-only => zeros.\n",
    "    token_type_ids = [0] * len(input_ids)\n",
    "\n",
    "    labels = list(input_ids)\n",
    "    prompt_len = min(len(pref[\"input_ids\"]), len(labels))\n",
    "    for i in range(prompt_len):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86774ecb",
   "metadata": {},
   "source": [
    "## 7) Collator (pad-safe for labels + token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CausalLMPadCollator:\n",
    "    pad_token_id: int\n",
    "    label_pad_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "\n",
    "        input_ids, attn, ttype, labels = [], [], [], []\n",
    "        for f in features:\n",
    "            ids = f[\"input_ids\"]\n",
    "            am = f.get(\"attention_mask\", [1] * len(ids))\n",
    "            lb = f[\"labels\"]\n",
    "            tt = f.get(\"token_type_ids\", [0] * len(ids))\n",
    "\n",
    "            pad_n = max_len - len(ids)\n",
    "            input_ids.append(ids + [self.pad_token_id] * pad_n)\n",
    "            attn.append(am + [0] * pad_n)\n",
    "            ttype.append(tt + [0] * pad_n)\n",
    "            labels.append(lb + [self.label_pad_id] * pad_n)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(ttype, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573bfe5",
   "metadata": {},
   "source": [
    "## 8) LoRA target-module selection\n",
    "Filters out vision/image/encoder paths and targets common projection layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ccd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_target_module_paths(model):\n",
    "    suffixes = (\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\")\n",
    "    banned_tokens = (\"vision\", \"visual\", \"image\", \"encoder\")\n",
    "\n",
    "    targets = []\n",
    "    for name, _module in model.named_modules():\n",
    "        leaf = name.split(\".\")[-1]\n",
    "        if leaf in suffixes:\n",
    "            lname = name.lower()\n",
    "            if any(bt in lname for bt in banned_tokens):\n",
    "                continue\n",
    "            targets.append(name)\n",
    "\n",
    "    if not targets:\n",
    "        for name, _ in model.named_modules():\n",
    "            leaf = name.split(\".\")[-1]\n",
    "            if leaf in (\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"):\n",
    "                lname = name.lower()\n",
    "                if any(bt in lname for bt in banned_tokens):\n",
    "                    continue\n",
    "                targets.append(name)\n",
    "\n",
    "    if not targets:\n",
    "        return [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "def _supports_trainingargs_kw(cls, kw: str) -> bool:\n",
    "    try:\n",
    "        import inspect\n",
    "        return kw in inspect.signature(cls.__init__).parameters\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c54314a",
   "metadata": {},
   "source": [
    "## 9) Environment + model/processor load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79568e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Safety checks ---\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemExit(\"CUDA not available.\")\n",
    "\n",
    "patch_torch_checkpoint_default_use_reentrant_false()\n",
    "\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "set_all_seeds(int(CFG[\"seed\"]))\n",
    "\n",
    "token = get_hf_token(CFG.get(\"hf_token\", \"\"))\n",
    "compute_dtype = pick_compute_dtype()\n",
    "print(\"compute_dtype:\", compute_dtype)\n",
    "\n",
    "# Processor\n",
    "processor = try_with_token(AutoProcessor.from_pretrained, CFG[\"model_name\"], token=token)\n",
    "tok = getattr(processor, \"tokenizer\", None)\n",
    "if tok is None:\n",
    "    raise SystemExit(\"AutoProcessor did not expose a tokenizer. Please upgrade transformers.\")\n",
    "\n",
    "# Pad token\n",
    "if tok.pad_token_id is None:\n",
    "    if tok.eos_token_id is None:\n",
    "        raise SystemExit(\"Tokenizer has no pad_token_id and no eos_token_id; cannot pad.\")\n",
    "    tok.pad_token = tok.eos_token\n",
    "pad_id = tok.pad_token_id\n",
    "\n",
    "print_gpu_mem(\"[before load] \")\n",
    "\n",
    "# Model (NO quantization)\n",
    "model = try_with_token(\n",
    "    AutoModelForImageTextToText.from_pretrained,\n",
    "    CFG[\"model_name\"],\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=compute_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=token,\n",
    ")\n",
    "\n",
    "print_gpu_mem(\"[after load]  \")\n",
    "\n",
    "# Hard-disable use_cache everywhere\n",
    "force_disable_use_cache_everywhere(model)\n",
    "\n",
    "# Enable gradient checkpointing (silence transformers warning while enabling)\n",
    "with suppress_use_cache_gc_warning():\n",
    "    enable_gc_no_reentrant(model)\n",
    "\n",
    "# Belt + suspenders\n",
    "force_disable_use_cache_everywhere(model)\n",
    "\n",
    "# Helps some backbones under GC\n",
    "if hasattr(model, \"enable_input_require_grads\"):\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "print(\"Loaded model + processor OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18574a9",
   "metadata": {},
   "source": [
    "## 10) Attach LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = pick_target_module_paths(model)\n",
    "print(\"LoRA target modules count:\", len(target_modules))\n",
    "print(\"LoRA target sample:\", target_modules[:8])\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=int(CFG[\"lora_r\"]),\n",
    "    lora_alpha=int(CFG[\"lora_alpha\"]),\n",
    "    lora_dropout=float(CFG[\"lora_dropout\"]),\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "# After PEFT wrap, hard-disable again\n",
    "force_disable_use_cache_everywhere(model)\n",
    "\n",
    "try:\n",
    "    model.print_trainable_parameters()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee79720",
   "metadata": {},
   "source": [
    "## 11) Load datasets + tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(\"json\", data_files={\"train\": CFG[\"train_jsonl\"]})[\"train\"].shuffle(seed=int(CFG[\"seed\"]))\n",
    "\n",
    "if not bool(CFG.get(\"no_eval\", False)):\n",
    "    val_ds = load_dataset(\"json\", data_files={\"val\": CFG[\"val_jsonl\"]})[\"val\"]\n",
    "else:\n",
    "    val_ds = None\n",
    "\n",
    "train_tok = train_ds.map(\n",
    "    lambda ex: tokenize_fn(processor, ex, int(CFG[\"max_len\"])),\n",
    "    remove_columns=train_ds.column_names,\n",
    "    desc=\"Tokenizing train\",\n",
    ")\n",
    "\n",
    "if val_ds is not None:\n",
    "    eval_len = int(CFG[\"max_len\"]) if int(CFG[\"eval_max_len\"]) <= 0 else int(CFG[\"eval_max_len\"])\n",
    "    val_tok = val_ds.map(\n",
    "        lambda ex: tokenize_fn(processor, ex, eval_len),\n",
    "        remove_columns=val_ds.column_names,\n",
    "        desc=\"Tokenizing val\",\n",
    "    )\n",
    "else:\n",
    "    val_tok = None\n",
    "\n",
    "collator = CausalLMPadCollator(pad_token_id=pad_id)\n",
    "\n",
    "print(\"Tokenized train:\", len(train_tok))\n",
    "print(\"Tokenized val:\", (len(val_tok) if val_tok is not None else \"disabled\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1554dc76",
   "metadata": {},
   "source": [
    "## 12) TrainingArguments + Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e777f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_strategy = \"no\" if bool(CFG.get(\"no_eval\", False)) else (\"epoch\" if int(CFG[\"eval_steps\"]) == 0 else \"steps\")\n",
    "save_strategy = \"epoch\" if int(CFG[\"eval_steps\"]) == 0 else \"steps\"\n",
    "\n",
    "steps_per_epoch = math.ceil(len(train_tok) / max(1, int(CFG[\"batch\"]) * int(CFG[\"grad_accum\"])))\n",
    "total_steps = max(1, steps_per_epoch * max(1, int(CFG[\"epochs\"])))\n",
    "warmup_steps = int(total_steps * float(CFG[\"warmup_ratio\"]))\n",
    "\n",
    "ta_kwargs = dict(\n",
    "    output_dir=CFG[\"out_dir\"],\n",
    "    num_train_epochs=int(CFG[\"epochs\"]),\n",
    "    per_device_train_batch_size=int(CFG[\"batch\"]),\n",
    "    gradient_accumulation_steps=int(CFG[\"grad_accum\"]),\n",
    "    learning_rate=float(CFG[\"lr\"]),\n",
    "    logging_steps=int(CFG[\"log_steps\"]),\n",
    "    save_total_limit=2,\n",
    "    bf16=(compute_dtype == torch.bfloat16),\n",
    "    fp16=(compute_dtype == torch.float16),\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=str(CFG[\"scheduler\"]),\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# ---- Eval/Save strategy kw compat ----\n",
    "if _supports_trainingargs_kw(TrainingArguments, \"eval_strategy\"):\n",
    "    ta_kwargs[\"eval_strategy\"] = eval_strategy\n",
    "else:\n",
    "    ta_kwargs[\"evaluation_strategy\"] = eval_strategy\n",
    "\n",
    "if _supports_trainingargs_kw(TrainingArguments, \"save_strategy\"):\n",
    "    ta_kwargs[\"save_strategy\"] = save_strategy\n",
    "\n",
    "# ---- Eval memory reducers (only if eval is enabled) ----\n",
    "if not bool(CFG.get(\"no_eval\", False)):\n",
    "    if _supports_trainingargs_kw(TrainingArguments, \"per_device_eval_batch_size\"):\n",
    "        ta_kwargs[\"per_device_eval_batch_size\"] = 1\n",
    "\n",
    "    if _supports_trainingargs_kw(TrainingArguments, \"fp16_full_eval\"):\n",
    "        ta_kwargs[\"fp16_full_eval\"] = (compute_dtype == torch.float16)\n",
    "    if _supports_trainingargs_kw(TrainingArguments, \"bf16_full_eval\"):\n",
    "        ta_kwargs[\"bf16_full_eval\"] = (compute_dtype == torch.bfloat16)\n",
    "\n",
    "    if _supports_trainingargs_kw(TrainingArguments, \"prediction_loss_only\"):\n",
    "        ta_kwargs[\"prediction_loss_only\"] = True\n",
    "\n",
    "    ta_kwargs[\"load_best_model_at_end\"] = True\n",
    "    ta_kwargs[\"metric_for_best_model\"] = \"eval_loss\"\n",
    "    ta_kwargs[\"greater_is_better\"] = False\n",
    "\n",
    "    if int(CFG[\"eval_steps\"]) != 0:\n",
    "        ta_kwargs[\"eval_steps\"] = int(CFG[\"eval_steps\"])\n",
    "        ta_kwargs[\"save_steps\"] = int(CFG[\"eval_steps\"])\n",
    "else:\n",
    "    ta_kwargs[\"load_best_model_at_end\"] = False\n",
    "\n",
    "train_args = TrainingArguments(**ta_kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# Final force-disable just before training\n",
    "force_disable_use_cache_everywhere(trainer.model)\n",
    "\n",
    "print(\"Trainer ready.\")\n",
    "print_gpu_mem(\"[before train] \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f23300",
   "metadata": {},
   "source": [
    "## 13) Train + save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "print_gpu_mem(\"[after train]  \")\n",
    "\n",
    "# Save adapter + processor\n",
    "model.save_pretrained(CFG[\"out_dir\"])\n",
    "processor.save_pretrained(CFG[\"out_dir\"])\n",
    "print(f\"[ok] saved LoRA adapter -> {CFG['out_dir']}\")\n",
    "print(\"[ok] training logs at:\", os.path.join(CFG[\"out_dir\"], \"trainer_state.json\"))\n",
    "\n",
    "# Optional merge\n",
    "if bool(CFG.get(\"save_merged\", False)):\n",
    "    merged_dir = os.path.join(CFG[\"out_dir\"], \"merged\")\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    try:\n",
    "        merged = model.merge_and_unload()\n",
    "        merged.save_pretrained(merged_dir, safe_serialization=True)\n",
    "        processor.save_pretrained(merged_dir)\n",
    "        print(f\"[ok] saved merged full model -> {merged_dir}\")\n",
    "    except Exception as e:\n",
    "        print(\"[warn] merge failed (adapter still saved). Error:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472d282",
   "metadata": {},
   "source": [
    "## 14) Quick notes\n",
    "- If you hit **eval OOM**, set `CFG['no_eval']=True` or reduce `CFG['eval_max_len']`.\n",
    "- If training OOM, reduce `max_len`, increase `grad_accum`, or reduce `batch`.\n",
    "- Make sure your `hf_auth.py` is in the same folder as this notebook (or installed as a module)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
